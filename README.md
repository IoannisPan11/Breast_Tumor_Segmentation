# Breast_Tumor_Segmentation
Simple U-net-based tool for segmenting tumors on digital mammograms


This repository is created for the purposes of my thesis and has the following summary:

Breast cancer is a serious threat to women worldwide. It is one of the most commonly diagnosed types of cancer, with a mortality rate of around one in six women, making it one of the leading causes of death for women globally. A very effective medical imaging technique for breast cancer detection, especially at an early stage, in which is more treatable, is mammography. Mammography is a cheap and safe method, performed by radiologists with the use of a dedicated X-ray, in order to visualize the breast tissue. It is used to support early treatment for breast cancer patients and to increase their survival rate, while aiming to avoid an unwanted aggressive solution, such as mastectomy. In the last two decades, various Computer-aided-detection (CAD) systems have been developed to help medical experts detect breast abnormalities, including masses, calcifications, architectural distortion of the tissue, and asymmetries in mammograms. In this work, a Convolutional Neural Network (CNN) for breast tumor segmentation on digital mammograms is proposed. The specific type of CNN used is the UNET, with the starting (input) image dimension being 256x256 pixels. More analytically, the INbreast dataset, which consists of 410 mammograms and their corresponding masks, was utilized to train, and evaluate the model, while the study is divided into two distinct approaches, in which were conducted eight experiments in total, meaning four in each approach. Former to the distinction between the two approaches, is preceded the preprocessing procedure, which contains the cropping, resizing, and normalization of the images. Concerning the two approaches, the first is about the model being trained with the “simple” INbreast dataset, while the second is trained with an augmented dataset, six times bigger than the initial one, resulting in a total of 2460 mammograms and their corresponding masks. The augmentation techniques selected include the histogram equalization, the gamma correction, and the 180-degree rotation of the images. The selected evaluation metrics that were used to calculate the performance of the model are the F1 score and the loss value. In both approaches, the dataset is split into three different sets, the training set, the validation set, and the testing set, with 70%, 20%, 10%, respectively. The optimizer selected for the fine-tuning of the model is the Adam. As previously noted, in each approach there have been done four experiments, which differ in the combination of learning rate and epochs. More specifically, for each approach there were conducted two 50-epoch experiments: one with a small learning rate (lr = 0.001) and one with a big learning rate (lr = 0.01), and two 100-epoch experiments: one with a small learning rate (lr = 0.001) and one with a big learning rate (lr = 0.01). Concerning the results of the study, it is noted that the second approach proved to be significantly better than the first. In fact, the unsatisfactory results of the first approach lead to the idea of implementing the data augmentation techniques and the second approach. More analytically, the best performance in the first approach was achieved by the fourth experiment, the one with the 100 epochs and the big learning rate, which achieved an F1 score of 0.64 for the testing set. The rest of the experiments didn’t manage to “learn” from the data, resulting in an F1 score in all three sets less than 0.60, and a bigger error value. After deep analysis of these results, the conclusion was that the dataset size is not enough to effectively train the model, since in three out of four experiments the model faced overfitting, a classic problem of small datasets. Thus, there was selected to check the performance of the model with a dataset of a bigger size. In the second approach, the more accurate example was the third, meaning the one with the 100 epochs and the small learning rate, which achieved an F1 score of 0.81 in the testing set. Moreover, the three rest experiments of the second approach passed the F1 score of 0.60 at the testing set that the three first experiments of the first approach failed to do. Their F1 score was 0.71, 0.64, and 0.77 respectively. Therefore, based on the above results it is proved that the selection of augmentation methods is successful and that the model indeed needs a larger number of data to train effectively. Additionally, it is shown that the selection of the learning rate depends primarily on the dataset, but also on the set number of epochs for model’s training. In the first approach two learning rates were not consistent on the two different options of epochs, since in 50-epoch experiments the small learning rate was more successful, but on the 100-epoch experiments the big learning rate achieved better results. On the contrary, in the second approach, in both 50 and 100 epochs experiments, the best performance of the model was achieved by the small learning rate.


INBreast dataset:
-----------------
Inês C. Moreira, Igor Amaral, Inês Domingues, António Cardoso, Maria João Cardoso, Jaime S. Cardoso,
INbreast: Toward a Full-field Digital Mammographic Database,
Academic Radiology,
Volume 19, Issue 2,
2012,
Pages 236-248,
ISSN 1076-6332,
https://doi.org/10.1016/j.acra.2011.09.014.
(https://www.sciencedirect.com/science/article/pii/S107663321100451X)
